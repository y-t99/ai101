## 上下文学习综述[^1]【2301】

### Abstract

随着大语言模型 (LLM) 能力的不断提升，上下文学习 (In-context Learning, ICL) 已成为自然语言处理 (NLP) 的新范式。在该范式中，大语言模型基于一个通过少量示例增强的上下文进行预测。探索 ICL 以评估并外推 LLM 的能力，已成为一个重要的研究趋势。本文旨在综述和总结 ICL 的研究进展与挑战。我们首先给出了 ICL 的形式化定义，并阐明其与相关研究的关联；接着，我们组织并讨论了多种先进技术，包括训练策略、提示设计策略及相关分析；此外，我们还探索了 ICL 的各种应用场景，如数据工程和知识更新。最后，我们指出了 ICL 面临的挑战，并对未来研究方向提出了建议。我们希望这项工作能激励更多旨在揭示 ICL 工作原理并加以改进的研究。

### Introduction

随着模型和数据规模的持续扩张 (Brown et al., 2020; Chowdhery et al., 2023; OpenAI, 2023; Touvron et al., 2023a,b)，大语言模型 (LLM) 展现出了上下文学习 (ICL) 的能力，即从上下文包含的少量示例中进行学习。许多研究表明，LLM 可通过 ICL 执行一系列复杂任务，例如解决数学推理问题 (Wei et al., 2022c)。这些强大的能力已被广泛验证为大语言模型的一种新兴能力 (Wei et al., 2022b)。

上下文学习的核心思想是“通过类比进行学习”。图 1 描述了语言模型如何通过 ICL 进行决策。首先，ICL 需要少量演示示例来构成提示上下文 (Prompt Context)，这些示例通常采用自然语言模板编写。然后，ICL 将用户的查询问题与提示上下文拼接成最终的输入，并将其送入语言模型进行预测。与监督学习不同，监督学习需要一个训练阶段，通过反向传播梯度来更新模型参数，而 ICL 则不涉及任何参数更新。模型被期望能够学习到演示中隐藏的模式，并据此作出正确预测。

![image-20250728211916569](./picture/icl-few-shot.png)

> 图 1：上下文学习 (ICL) 示意图。ICL 需要一个包含少量演示示例的提示上下文，这些示例通常用自然语言模板编写。大语言模型将此提示和用户查询作为输入，并负责作出预测。

作为一个新兴范式，ICL 具有多项显著优势。首先，由于演示示例是用自然语言编写的，它为与 LLM 的交互提供了一个可解释的接口 (Brown et al., 2020)。该范式使得通过修改演示和模板来将人类知识融入 LLM 变得异常便捷 (Liu et al., 2022; Lu et al., 2022; Wei et al., 2022c; Wu et al., 2023b)。其次，上下文学习与人类通过类比进行学习的决策过程有相似之处 (Winston, 1980)。第三，与监督学习相比，ICL 是一个“免训练”的学习框架。这不仅能极大地降低模型适应新任务所需的计算成本，也使得“语言模型即服务” (Language-Model-as-a-Service) (Sun et al., 2022) 成为可能，从而能够轻松应用于大规模的真实世界任务。

尽管 ICL 前景广阔，但其中仍存在一些有趣的问题和特殊的性质需要深入探究。虽然一系列基础的 GPT 模型已展现出卓越的 ICL 能力，但多项研究发现，通过在预训练阶段进行适应性调整，该能力可以得到显著提升 (Min et al., 2022b; Li et al., 2024c)。此外，ICL 的性能对具体设置高度敏感，这些设置包括提示模板、演示示例的选择与顺序，以及其他多种因素 (Wang et al., 2023e; Liu et al., 2024b)。同时，优化演示示例的简洁性以及提升 ICL 的计算效率，也是当前研究的关键领域 (Liu et al., 2024a)。更进一步说，尽管已有一些初步的解释 (Dai et al., 2023a; Jiang, 2023)，但 ICL 的深层工作机制至今仍不清晰，有待进一步的探索。

随着 ICL 相关研究的迅速增长，本综述旨在帮助学术界了解该领域的最新进展。在接下来的章节中，我们将深入探讨相关研究，并在图 2 和附录 A 中分别对现有工作进行分类和总结关键发现。我们着重指出了当前面临的挑战和潜在的研究方向，希望能为该领域的初学者提供一份实用的学习路线图，并为未来的研究带来启发。

---

图 2 上下文学习 思维导图

1. Training (训练)

- Pre-training (§3.1) (预训练)
  - PICL (Gu et al., 2023), MEND (Li et al., 2024c), ICLM (Shi et al., 2024)
- Warmup (§3.2) (预热)
  - MetaICL (Min et al., 2022b), OPT-IML (Iyer et al., 2022), Super-NaturalInstructions (Wang et al., 2022b), FLAN (Wei et al., 2022a), Scaling Instruction (Chung et al., 2022), Self-supervised ICL (Chen et al., 2022), Symbol Tuning (Wei et al., 2023a), RICL (Chu et al., 2023), ICL Markup (Brunet et al., 2023)

2. Inference (推理)

- Demonstration (§4.1) (演示)
  - Selection (§4.1.1) (选择)
    - Unsupervised (无监督)
      - KATE (Liu et al., 2022), SG-ICL (Kim et al., 2022), Self-Adaptive (Wu et al., 2023b), PPL (Gonen et al., 2023), MI (Sorensen et al., 2022), Informative Score (Li and Qiu, 2023), IDS (Qin et al., 2023), Votek (Su et al., 2023)
    - Supervised (有监督)
      - EPR (Rubin et al., 2022), Q-Learning (Zhang et al., 2022a), AdaICL (Mavromatis et al., 2023), Topic (Wang et al., 2023e), UDR (Li et al., 2023d)
  - Reformatting (§4.1.2) (格式化)
    - SG-ICL (Kim et al., 2022), Structured Prompting (Hao et al., 2022b), AutoICL (Yang et al., 2023a), WICL (Yang et al., 2023b), ICV (Liu et al., 2024a)
  - Ordering (§4.1.3) (排序)
    - GlobalE&LocalE (Lu et al., 2022), ICCL (Liu et al., 2024b)
- Instruction (§4.2) (指令)
  - Instruction Induction (Honovich et al., 2023), Self-Instruct (Wang et al., 2023f), APE (Zhou et al., 2023c), Grimoire (Chen et al., 2024)
- Scoring Function (§4.3) (评分函数)
  - Calibrate (Zhao et al., 2021), Channel Models (Min et al., 2022a), kNN-Prompting (Xu et al., 2023a)

3. Analysis (分析)

- Influencing Factors (§5.1) (影响因素)
  - Pre-training Stage (§5.1.1) (预训练阶段)
    - Pre-Training Data (预训练数据)
      - Distribution (Chan et al., 2022; Wies et al., 2023), Domain (Shin et al., 2022; Han et al., 2023b), Diversity (Yadlowsky et al., 2023)
    - Model and Training (模型与训练)
      - Architecture (Ding et al., 2024), Pre-training steps (Wei et al., 2022b), Parameters (Brown et al., 2020; Wei et al., 2022b)
  - Inference Stage (§5.1.2) (推理阶段)
    - Input Labels (输入标签)
      - Mapping (Yoo et al., 2022; Pan et al., 2023a; Tang et al., 2023a), Settings (Min et al., 2022c)
    - Demonstration Examples (演示示例)
      - Diversity and Simplicity (An et al., 2023), Query Similarity (Liu et al., 2022; An et al., 2023), Feature bias (Si et al., 2023), Order (Lu et al., 2022; Zhang et al., 2022b; Liu et al., 2023b)
- Learning Mechanism (§5.2) (学习机制)
  - Functional Modules (§5.2.1) (功能模块)
    - Induction Heads (Olsson et al., 2022; Bietti et al., 2023), Computational Layers (Wang et al., 2023b), Attention Modules (Li et al., 2023c)
  - Theoretical Interpretation (§5.2.2) (理论解释)
    - Bayesian Framework (Xie et al., 2022; Wang et al., 2023e; Jiang, 2023), Gradient Descent (Dai et al., 2023a; Irie et al., 2022; Mahankali et al., 2023), Others (Garg et al., 2022; Akyürek et al., 2023; Li et al., 2023c; Pan et al., 2023b)
---

### Definition and Formulation

遵循 Brown et al. (2020) 的工作，我们在此给出上下文学习的形式化定义：

In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration.
上下文学习是一种允许语言模型仅通过少量演示示例就能学会执行任务的范式。

形式化地，给定一个查询输入文本 _x_ 和一组候选答案 _Y_ = {y<sub>1</sub>, . . . , y<sub>m</sub>}，一个预训练的语言模型 _M_ 会以一个演示集 _C_ 为条件，将得分最高的候选答案作为预测结果。演示集 _C_ 包含一个可选的任务指令 _I_ 和 _k_ 个演示示例，因此 _C_ = {_I_, _s_(x<sub>1</sub>, y<sub>1</sub>), . . . , _s_(x<sub>k</sub>, y<sub>k</sub>)} 或 _C_ = {_s′_ (x<sub>1</sub>, y<sub>1</sub>, _I_), . . . , _s′_ (x<sub>k</sub>, y<sub>k</sub>, _I_)}，其中 _s′_ (x<sub>i</sub> , y<sub>i</sub> , _I_) 是一个根据具体任务用自然语言编写的示例。根据这 _k_ 个演示示例是否属于同一任务，ICL 可被分为特定于任务的 ICL (task-specific ICL) 和跨任务的 ICL (cross-task ICL)。在后一种情况中，不同的示例拥有各自独立的任务指令。对于一个候选答案 y<sub>j</sub>，其似然概率来自于一个评分函数 _f_ 对整个输入序列的计算：

$$
P(y_j | x) \triangleq f_M(y_j, C, x)
$$

> 给了模型上下文（C）和新问题（x）之后，某个候选答案（yⱼ，比如“积极”）是正确答案的可能性有多大。

最终预测的标签 ŷ 是具有最高概率的候选答案：

$$
\hat{y} = \underset{y_j \in Y}{\arg \max} P(y_j | x).
$$

> 模型在所有选项中进行“择优录取”的数学表达方式。它计算每个选项的得分，然后选择得分最高的那个选项作为它的最终答案。

根据该定义，我们可以看到 ICL 与以下几个相关概念有所不同：
(1) **提示学习 (Prompt Learning)**：提示 (Prompt) 可以是离散的模板，也可以是软参数 (soft parameters)，其作用是引导模型预测出期望的输出。ICL 可被视为提示调优 (prompt tuning) 的一个子类，其中演示示例是提示的一部分。Liu et al. (2023c) 对提示学习进行了全面的综述，但他们的研究未包含 ICL。
(2) **小样本学习 (Few-shot Learning)**：小样本学习是一种通用的机器学习方法，它需要通过少量有监督的样本来调整模型参数以执行特定任务 (Wang and Yao, 2019)。相比之下，ICL 不需要更新参数，而是直接在预训练好的大语言模型上执行。

### Model Training

尽管大语言模型已经直接展示了有前景的 ICL 能力，但许多研究表明，这些 ICL 能力可以通过推理前的专门训练得到进一步增强（Chen et al., 2022; Gu et al., 2023; Shi et al., 2024）。

![image-20250729010354276](./picture/icl-model-training.png)

#### Pretraining

一个提升大语言模型（LLMs）上下文学习（ICL）能力的直接方向是通过预训练或持续预训练。例如，Gu et al. (2023) 和 Shi et al. (2024) 提出通过聚合相关的上下文来重组预训练语料库，使模型学会在先前的演示之间进行推理。不同的是，Li et al. (2024c) 引入了一种元蒸馏预训练过程，该过程允许大语言模型利用被蒸馏后的演示向量进行推理，从而在不影响其效果的前提下提升ICL的效率。


#### Warmup

另一种增强ICL能力的方法是在预训练和ICL推理之间增加一个持续训练阶段，我们简称为模型预热（Warmup）。预热是ICL的一个可选流程，它通过修改或增加参数的方式在推理前对大语言模型进行调整。

由于大多数预训练数据并非为ICL量身定制 (Chen et al., 2022)，研究人员引入了多种预热策略来弥合预训练与ICL推理之间的差距。Min et al. (2022b) 和 Wang et al. (2022b) 都提出在包含多个演示示例的广泛任务上持续微调大语言模型，从而提升其ICL能力。为了促使模型从上下文中学习输入与标签的映射关系，Wei et al. (2023a) 提出了符号微调（symbol tuning），该方法用任意符号（例如，“foo/bar”）来替代自然语言标签（例如，“正面/负面情感”）。Chen et al. (2022) 提出了一种自监督方法，用以将原始文本与下游任务中的ICL格式对齐。此外，多项研究已经指明了指令的潜在价值 (Mishra et al., 2021; Wei et al., 2022a)。通过在超过60个经由自然语言指令模板表达的数据集上对1370亿参数的LaMDA-PT (Thoppilan et al., 2022) 进行微调，FLAN模型 (Wei et al., 2022a) 提升了大语言模型遵循指令的能力，从而增强了其零样本和少样本ICL的性能。Chung et al. (2022) 和 Wang et al. (2022b) 提出使用超过 1000 个任务指令来进一步扩大指令微调的规模。

### Prompt Designing

在本节中，我们重点关注在推理（inference）阶段进行上下文学习（In-Context Learning, ICL）的原理，包括示例组织（demonstration organization）（§4.1）和指令格式化（instruction formatting）（§4.2）。

#### Demonstration Organization

许多研究表明，上下文学习（ICL）的性能强烈依赖于其示例的呈现形式（demonstration surface），这包括示例的选择（selection）、格式化（formatting）和排序（ordering）（Zhao et al., 2021; Lu et al., 2022）。在本小节中，我们对示例组织（demonstration organization）策略进行了综述，并将其归为三类，如表 1 所示。

|   分类   |                方法                | 示例来源 |         模型         |              特点               |
| :------: | :--------------------------------: | :------: | :------------------: | :-----------------------------: |
| 示例选择 |      KATE (Liu et al., 2022)       |   人类   |        GPT-3         |          KNN Selection          |
|          |     MI (Sorensen et al., 2022)     |   人类   |        GPT-3         |       Mutual Information        |
|          |      EPR (Rubin et al., 2022)      |   人类   |   GPT-{J, 3}/CodeX   |      Score-based Retrieval      |
|          |       IDS (Qin et al., 2023)       |   人类   |       GPT-3.5        |       Iterative Selection       |
|          |  AdaICL (Mavromatis et al., 2023)  |   人类   |     GPT-{J, Neo}     |     Selective Demonstration     |
|          |       UDR (Li et al., 2023d)       |   人类   |     GPT-Neo-2.7B     |        Unified Retrieval        |
| 示例格式 |     SG-ICL (Kim et al., 2022)      | 模型生成 |        GPT-J         |  Auto Demonstration Generation  |
|          |    AutoICL (Yang et al., 2023a)    | 模型生成 |  GPT-3.5-Turbo-0301  |    Reasoning Path Generation    |
|          |      MSP (Yang et al., 2023b)      |   人类   |      GPT series      | Adjusting Demonstration Weight  |
|          |      ICV (Liu et al., 2024a)       |   人类   | Falcon-7b / Llama-7b |     Demonstration Embedding     |
| 示例排序 | GlobalE & LocalE (Lu et al., 2022) |   人类   |      GPT-{2, 3}      |      Best Order Selection       |
|          |      ICCL (Liu et al., 2024b)      |   人类   | Llama2/Mixtral/Qwen  | Ordering from Simple to Complex |

##### Demonstration Selection

演示选择旨在回答一个根本性问题：哪些样本是上下文学习（ICL）的优质示例？我们将相关研究划分为两大类：基于预定义指标的无监督方法和有监督方法。

**无监督方法 (Unsupervised Method)** 一种直接的 ICL 示例选择方法是，根据输入实例与候选示例的相似度，从中选出最近邻的示例 (Liu et al., 2022; Tanwar et al., 2023; Qin et al., 2023)。为此，通常会使用基于句子嵌入的 L2 距离或余弦相似度等度量指标。例如，Liu et al. (2022) 提出了 KATE，这是首个用于选择上下文学习示例的、基于 kNN 的无监督检索器。类似地，在多语言 ICL 任务中，可以检索 kNN 跨语言演示来增强源语言与目标语言的对齐 (Tanwar et al., 2023)。Su et al. (2023) 提出结合图与置信度分数来选择具有多样性和代表性的示例。除了距离度量，互信息 (Sorensen et al., 2022) 和困惑度 (Gonen et al., 2023) 也已被证明，在无需标注样本或特定大语言模型的场景下，对于提示选择颇具价值。此外，将大语言模型的输出分数用作无监督指标，在演示选择中也展现了其有效性 (Wu et al., 2023b; Nguyen and Wong, 2023; Li and Qiu, 2023)。特别地，Wu et al. (2023b) 基于数据传输所需的编码长度，从 kNN 示例中选择了最佳的子集排列，从而在给定 *x* 和 *C* 的条件下对标签 *y* 进行压缩。Li and Qiu (2023) 则使用 infoscore，即在验证集中的所有 (*x*, *y*) 对上，对 *P* (*y*|*x*<sub>i</sub>, *y*<sub>i</sub>, *x*) / *P* (*y*|*x*) 求取平均值，并引入了多样性正则化项。

> **场景比喻：**
> 你是一个要参加考试的学生（这个学生就是**大语言模型LLM**）。你手上有一道新题（**输入实例 x**），但你不太会做。你的旁边有一大堆参考书，里面全是例题和答案（**候选示例集**）。为了快速学会解这道新题，你不能把所有参考书都看一遍，只能挑出**最好、最相关**的几道例题（**ICL演示示例**）来看。
>
> 那么，**如何自动地、智能地挑出这几道“神级”例题呢？**
> 这段话讲的就是“**无监督**”的方法，意思是**没有任何老师（人工标注）提前告诉你哪些例题是“好”的**，你必须自己发明一套规则来挑。
>
> 下面，我们来逐一拆解这段话里提到的几种“挑例题”的自动化方法：
>
> ------
>
> 方法一：物以类聚，找最相似的 (k-Nearest Neighbors, kNN)
>
> - **原文描述**：“...根据输入实例与候选示例的相似度，从中选出最近邻的示例...通常会使用基于句子嵌入的L2距离或余弦相似度等度量指标。”
> - **通俗讲解**：这是最直观的方法。既然你的新题是关于“微积分”的，那你肯定会去参考书里找“微积分”的例题，而不是“线性代数”的。
> - **技术实现**：
>   1. **句子嵌入 (Sentence Embeddings)**：计算机无法直接理解文字。所以，它先把你的“新题”和参考书里所有的“例题”都转换成一串数字，也就是数学上的**向量**（可以想象成三维空间里的坐标 (x, y, z)，只不过这里是几百甚至上千维）。这个转换过程就叫“嵌入”。
>   2. **L2距离 / 余弦相似度**：一旦都变成了向量（坐标），计算它们之间的“距离”或“夹角”就很容易了。
>      - **L2距离**：就是我们初中学的两点间距离公式，距离越近，说明题目越相似。
>      - **余弦相似度**：计算两个向量的夹角，夹角越小（趋近于0度），说明方向越一致，题目越相似。
>   3. **kNN**：通过计算距离/相似度，找到离你的“新题”**最近的k个**例题，把它们挑出来。
> - **提到的论文**：
>   - **KATE**：就是第一个用这种kNN方法来挑例题的系统。
>   - **多语言ICL**：这个想法同样适用于翻译。如果要翻译一句英文，就去找和它意思相近的“中英对照”例句。
> 
> 方法二：不仅要相似，还要有代表性 (Diversity and Representativeness)
> 
>- **原文描述**： “Su et al. (2023) 提出结合图与置信度分数来选择具有多样性和代表性的示例。”
> - **通俗讲解**：只找最相似的还不够。如果你挑出来的3道例题几乎一模一样，那看了等于白看。你希望挑出来的例题既和你的新题相关，又各自有特点，能覆盖不同的知识点（**多样性**），同时每道题本身质量要高，解法清晰（**代表性**）。
>- **技术实现**：这个方法更进一步，它不仅看单个例题，还看整个“例题组合”的质量。
> 
>方法三：从信息论和概率论里找智慧
> 
> 这部分比较抽象，但思想很巧妙。
> 
>- **原文描述 1**：“互信息...和困惑度...对于提示选择颇具价值。”
>   - **互信息 (Mutual Information)**：衡量“知道一个东西后，对另一个东西的不确定性减少了多少”。如果一道例题的“互信息”很高，意味着它能极大地帮助你理解新题，提供了关键信息。我们就挑这种“信息量大”的例题。
>  - **困惑度 (Perplexity)**：衡量模型对一件事的“惊讶程度”。如果模型看了几道例题后，再看到新题的正确答案时，它一点也不“困惑”或“惊讶”（困惑度很低），说明这几道例题选得非常好，让模型完全掌握了规律。我们的目标就是找到能让“困惑度”最小的例题组合。
> - **原文描述 2**：“将大语言模型的输出分数用作无监督指标...也展现了其有效性。”
>  - **通俗讲解**：这个方法非常“元认知”，就是**让模型自己来判断哪个例题好**。
>   - **技术实现**：我们试着把不同的例题喂给模型，然后看它做新题时的“自信程度”（即输出正确答案的概率分数）。如果看了某道例题A后，模型做对新题的概率大大提升，那说明例题A是个好例子。
>
> ------
> 
> 方法四：更高级的“自判别”方法（具体论文解读）
> 
> - **原文描述 1**：“Wu et al. (2023b) 基于数据传输所需的编码长度...对标签y进行压缩。”
>   - **核心思想 (MDL原理)**：这个思想来源于“最小描述长度”原理。简单说就是：**最好的解释是让事情变得最简单的解释**。
>   - **通俗讲解**：如果几道例题选得特别好，那么新题的答案（y）在这些例题的“上下文”（C）和新题题干（x）下，就变得**非常显而易见，一说就通**。在信息论里，“一说就通”意味着描述这个答案所需要的信息量（编码长度）非常短。所以，这个方法就是去寻找能让新题答案变得“最不需要解释”的例题组合。
> - **原文描述 2**：“Li and Qiu (2023) 则使用infoscore，即...P(y|x_i, y_i, x) / P(y|x) 求取平均值...”
>  - **核心思想**：直接量化一个例题的“帮助有多大”。
>   - **公式拆解**：
>    - P(y|x)：在**不看任何例题**的情况下，模型猜对答案的概率。这是我们的“基础水平”。
>         - P(y|x_i, y_i, x)：在看了**某一个例题(x_i, y_i)** 之后，模型猜对答案的概率。这是我们的“提升后水平”。
>    - **两者相除**：提升后水平 / 基础水平。这个比值越大，说明这个例题的“提分效果”越好。
>   - **infoscore**：把所有候选例题的“提分效果”都算一遍，然后挑分最高的那些。同时，为了避免选出来的例题太相似（参考方法二），它还加了一个“多样性正则化项”作为惩罚。
> 
> ---
> 
> 总而言之，这段话介绍了一系列“**自动化挑选优质学习范例**”的无监督技术。它们虽然听起来复杂，但都源于一些朴素的思想：
> 
> 1. **找相似的** (kNN)。
> 2. **找既相似又多样的** (Diversity)。
> 3. **找信息量大的、能降低模型困惑的** (Mutual Information, Perplexity)。
> 4. **让模型自己判断哪些例子帮助最大** (Output Scores, Infoscore)。
> 5. **找能让问题答案变得最“显然”的例子** (MDL)。

为了更直观地比较几种无监督方法的性能，我们选择 topk（Liu et al., 2022）、votek（Su et al., 2023）、mdl（Wu et al., 2023b）进行实验。结果如表 2 所示。实验的详细信息可以在附录 B 中找到。

| Model  | Method | SST5     | SST2     | CQA      | SNLI     | News     | Avg      |
| ------ | ------ | -------- | -------- | -------- | -------- | -------- | -------- |
| GPT2   | topk   | 40.1     | 74.9     | 30.2     | 39.7     | 62.7     | 49.5     |
|        | votek  | 32.4     | 51.0     | 29.8     | 35.8     | 25.5     | 34.9     |
|        | mdl    | **43.3** | **86.7** | **32.7** | **41.4** | **68.0** | **54.4** |
| GPT-J  | topk   | **46.9** | 84.6     | 58.4     | **60.7** | **69.1** | **63.9** |
|        | votek  | 33.8     | 87.3     | 63.4     | 43.1     | 25.3     | 50.6     |
|        | mdl    | 37.6     | **87.9** | **64.1** | 59.8     | 68.2     | 63.5     |
| Qwen2  | topk   | 54.1     | 83.3     | 76.3     | **68.2** | 64.9     | **69.4** |
|        | votek  | **55.3** | **86.9** | 76.1     | 51.6     | **65.3** | 67.0     |
|        | mdl    | 54.6     | 86.1     | **77.1** | 65.0     | 63.2     | 69.2     |
| Llama3 | topk   | 53.0     | **90.3** | 76.1     | **64.0** | 74.0     | **71.5** |
|        | votek  | 54.9     | 88.9     | 72.6     | 57.7     | **78.3** | 70.5     |
|        | mdl    | **54.4** | 89.1     | **76.5** | 59.9     | 74.6     | 70.9     |

**表2：演示选择方法的公平比较。** CQA 和 News 分别是 Commonsense QA (常识问答) 和 AG News 的缩写。最佳结果已用粗体标出。我们针对 topk (Liu et al., 2022)、votek (Su et al., 2023) 和 mdl (Wu et al., 2023b) 这几种方法进行的实验表明，ICL (上下文学习) 示例选择方法的有效性是依赖于模型的。在 GPT-2 上，mdl 方法表现最好，而在其他三个模型上，topk 方法表现最佳。

**有监督方法** 尽管现成的检索器为广泛的 NLP 任务提供了便利的服务，但由于缺乏任务特定的监督，它们是启发式的且次优的。为了解决这个问题，已经开发了许多有监督方法（Rubin et al., 2022; Ye et al., 2023; Wang et al., 2023e; Zhang et al., 2022a）。EPR（Rubin et al., 2022）引入了一种两阶段方法来训练用于演示选择的密集检索器。对于特定输入，它首先利用无监督方法（例如，BM25）来召回相似示例作为候选，然后使用这些数据构建有监督的密集检索器。继 EPR 之后，Li et al. (2023d) 采用了统一的演示检索器来选择不同任务的演示。与之前检索单个演示的工作不同，Ye et al. (2023) 提出检索整个演示集以建模示例之间的相互关系。此外，Mavromatis et al. (2023) 引入了 AdaICL，这是一种模型自适应方法，使用 LLM 预测未标记数据集，为每个实例生成不确定性分数。

>无监督方法就像“凭感觉”挑例题，虽然简单方便，但不一定挑得准。有监督方法则是**专门训练一个“挑选专家”（检索器），让它学会如何精准地挑选例题**。这个专家是通过大量“好坏”案例的学习才上岗的。
>
>为了解决“不够专业”的问题，研究者们开发了各种有监督的方法，本质上都是在**想办法训练一个更聪明的“挑选专家”（检索器）**。
>
>方法一：EPR - 先粗选，再精调
>
>- **原文**：“EPR...引入了一种两阶段方法来训练用于演示选择的密集检索器。对于特定输入，它首先利用无监督方法（例如，BM25）来召回相似示例作为候选，然后使用这些数据构建有监督的密集检索器。”
>- **比喻**：想象一下招聘一个顶尖销售员。
> 1. **第一阶段（粗选/召回）**：HR先用一个通用标准（比如学历、工作年限）从成千上万份简历中筛选出100份看起来还不错的（**无监督方法，如BM2T5**）。这一步的目的是快速缩小范围，把明显不相关的去掉。
> 2. **第二阶段（精调/训练）**：CEO（**大语言模型**）亲自面试这100个候选人。他会模拟真实的销售场景，让候选人现场推销产品。有的候选人表现好（能让CEO点头），有的表现差。通过观察这一轮表现，公司就总结出了一套识别人才的“火眼金睛”标准（**构建有监督的密集检索器**）。这个标准是针对“我们公司销售任务”量身定做的。
>- **技术流程**：
> 1. **召回 (Recall)**：用kNN或BM25（一种经典的文本匹配算法）这种快速的无监督方法，先从海量例题库里捞出一批“可能相关”的候选项。
> 2. **构建训练数据**：把这些候选项喂给大语言模型，看哪个候选项能让模型在验证集上表现最好。表现好的，我们就给它贴上“**好例子**”（正标签）的标签；表现差的，就贴上“**坏例子**”（负标签）的标签。
> 3. **训练 (Train)**：用这些带了“好/坏”标签的数据，去训练一个专门的“挑选专家”模型（**密集检索器 Dense Retriever**）。这个专家模型学会了如何区分好坏例子。
>- **“密集检索器”是什么？** 它也是一个神经网络，能把问题和例题都转换成向量（嵌入），但它的转换方式是通过“有监督”训练学来的，目的就是让“好例子”的向量离问题向量更近，“坏例子”的向量离得更远。
>
>---
>
>方法二：统一检索器 - 训练一个“通才专家”
>
>- **原文**：“继 EPR 之后，Li et al. (2023d) 采用了统一的演示检索器来选择不同任务的演示。”
>- **讲解**：EPR的方法是为一个特定任务训练一个专家。而这个方法更进一步，它试图训练一个**更全能的专家**，这个专家不仅能为“数学解题”挑例题，也能为“写诗”、“代码生成”等**多种不同任务**挑例题。
>
>---
>
>方法三：检索整个集合 - 挑一个“梦之队”，而不是单个明星球员
>
>- **原文**：“与之前检索单个演示的工作不同，Ye et al. (2023) 提出检索整个演示集以建模示例之间的相互关系。”
>- **讲解**：之前的方法都是给每个候选例题单独打分，然后选分最高的几个。但这忽略了一个重要问题：**例题之间是有化学反应的！**
>- **比喻**：组建一支篮球队。你不能只挑5个得分能力最强的后卫，这样肯定不行。你需要一个有后卫、有前锋、有中锋的**互补组合**。这个方法就是不再孤立地看每个例题，而是直接评估**一整个例题组合（比如3个或5个例题打包）**的质量，考虑它们之间的互补性和多样性。
>
>---
>
>方法四：AdaICL - 让模型自己当考官，生成“不确定性”标签
>
>- **原文**：“Mavromatis et al. (2023) 引入了 AdaICL，这是一种模型自适应方法，使用 LLM 预测未标记数据集，为每个实例生成不确定性分数。”
>- **讲解**：这是一个非常聪明的“自监督”方法，它巧妙地**自己创造了标签**。
>- **比喻**：老师让学生做一套全新的、没有答案的模拟卷。
> 1. **预测**：学生（**LLM**）先把这套卷子做一遍。
> 2. **生成不确定性分数**：做完后，学生对自己做的每一道题进行反思。有些题他非常确定自己做对了，有些题他很犹豫、没把握（**不确定性高**）。
> 3. **挑选**：那些让学生**最没把握**的题，往往是最有价值、最能暴露知识盲区的题。我们就把这些“高不确定性”的题挑选出来，作为最需要学习的“优质例题”。
>- **技术流程**：这种方法利用了大语言模型对某些数据点预测的“自信程度”来反向选择最有信息量的示例，这是一种非常高效的利用未标注数据的方式。
>
>---
>
>这段话的核心就是：为了更精准地给大语言模型挑选学习范例，我们不满足于“凭感觉”的无监督方法。我们采用**有监督**的思路，通过各种方式**训练一个专业的“挑选专家”（检索器）**。
>
>训练这个专家的方法五花八门，但万变不离其宗：
>
>- **EPR**：先海选，再精选训练。
>- **统一检索器**：训练一个能应对多项任务的通才专家。
>- **检索集合**：不只看个体，更看重团队组合的化学反应。
>- **AdaICL**：让模型自己做题，然后挑出那些它最“没把握”的题来重点学习。

基于提示调优（Prompt Tuning），Wang等人（2023e）提出了一种新颖的视角：他们将大语言模型（LLM）视为一个能够从少量演示中推断出任务核心概念 θ 的主题模型，并依据所推断出的概念来生成词元。为了在模型中表示这些潜在概念，他们引入了与任务相关的“概念词元”（concept tokens），并通过学习来调整这些词元，以最大化在给定概念θ和输入x的条件下生成正确答案y的概率，即*P*(*y*|*x*, *θ*)。在此框架下，演示示例的选择标准是：优先选择那些最能帮助模型推断出核心概念θ的示例，这一可能性由*P*(*θ*|*x*, *y*)来衡量。

##### Demonstration Reformatting

除了直接从训练数据中选取示例，另一大研究趋势是利用大语言模型 (LLM) 对已有演示的表示形式进行重构 (reformat) (Kim et al., 2022; ...)。例如，Kim et al. (2022) 提出直接由 LLM 生成演示，以减少对外部演示数据的依赖。**结构化提示 (Structured Prompting)** (Hao et al., 2022b) 提出采用特殊的位置嵌入来分别编码演示示例，再通过一种重缩放注意力机制 (rescaled attention mechanism) 将这些编码信息应用于测试示例。与其他方法不同，另一些研究则专注于修改演示的潜在表示 (latent representation) (Liu et al., 2024a; Li et al., 2024a)。具体来说，Liu et al. (2024a) 开发了**上下文内向量 (In-Context Vectors, ICV)**，这些向量源自 LLM 对演示示例的潜在嵌入。在推理时，ICV 被用来调整 LLM 的潜在状态，从而增强模型更有效效仿 (follow) 演示的能力。

> 教一个非常聪明但没经验的学生
>
> - **大语言模型 (LLM)**：想象成一个超级聪明的学生。他能过目不忘，举一反三，但你需要给他一些例子，他才知道你到底想让他做什么。
> - **你**：就是老师。
> - **演示 (Demonstration)**：就是你给学生做的“例题”。比如，你想教他做“英译中”的任务，你就会给他看几个例子：
>   - 例1: apple -> 苹果
>   - 例2: banana -> 香蕉
> - **新问题 (Test example)**：你给学生一道新的题目，看他会不会。比如，你问他 orange -> ?，期望他能答出 橘子。
>
> 这段话的核心思想是：**怎样给学生看“例题”，才能让他学得最好、最快？**
>
> 直接给他看书本上的例题（从训练数据中选取）是一种方法，但可能不是最高效的。于是，学者们研究出了几种更高级的“教学方法”，也就是所谓的 **“演示重构” (Demonstration Reformatting)**。
>
> ------
>
> 三种高级“教学方法”详解
>
> 这段话里提到了三种不同的高级方法，我们用上面的比喻来解释：
>
> 方法一：专门为学生“编”新例题 (Kim et al.)
>
> - **原文说**：“直接由 LLM 生成演示，以减少对外部演示数据的依赖。”
> - **比喻解释**：老师（你）懒得从一堆旧课本（训练数据）里找合适的例题了。你干脆请来一位更厉害的“金牌教师”（另一个更强大的 LLM），让他专门为这个学生量身定做几道最经典、最完美的例题。
> - **一句话总结**：**我们不找例题，我们创造例题。**
>
> ------
>
> 方法二：给例题“划重点并贴标签” (Hao et al.)
>
> - **原文说**：“采用特殊的位置嵌入... 再通过一种重缩放注意力机制...”
> - **比喻解释**：老师觉得光给例题还不够，得让学生知道哪些是重点。
>   1. **“特殊的位置嵌入”**：老师在发给学生的练习纸上，用不同颜色的荧光笔把例题1、例题2、例题3分别框起来。这些“颜色标记”就是“位置嵌入”，它在告诉学生：“注意，这是一个独立的例题单元！”
>   2. **“重缩放注意力机制”**：在学生做新题目前，老师特意叮嘱他：“我用荧光笔画出来的那些例题，你要**加倍、加倍地关注**！它们是解题的关键！” 这个“加倍关注”的指令，就是“重缩放注意力机制”。
> - **一句话总结**：**把每个例题都打上特殊标记，然后告诉模型要格外关注这些有标记的例题。**
>
> ------
>
> 方法三：提取“解题思路”直接注入学生大脑 (Liu et al., Li et al.)
>
> 这是最抽象，也是最“科幻”的一种方法。
>
> - **原文说**：“修改演示的潜在表示... 开发了上下文内向量 (ICV)... 在推理时，ICV 被用来调整 LLM 的潜在状态...”
> - **比喻解释**：
>   1. **“潜在表示”**：当学生看了例题后，他脑子里会形成一个模糊的“概念”或“感觉”，比如对“英译中”这个任务的整体理解。这个存在于他大脑深处的、非语言的“理解”，就是“潜在表示”。
>   2. **“上下文内向量 (ICV)”**：老师有一种黑科技，可以扫描学生的大脑，把他刚刚形成的那个“英译中”的“理解”（潜在表示）给抽出来，并浓缩成一剂“知识药剂”。这剂药剂就是“上下文内向量 (ICV)”。
>   3. **“调整 LLM 的潜在状态”**：当学生遇到新题目时，老师不再给他看原始的例题了，而是直接把这剂“知识药剂”(ICV) 注射进他的大脑。学生的思维模式立刻被调整到“英译中”频道。
> - **一句话总结**：**我们不给模型看例题本身，而是分析模型从例题中学到了什么，把这个“学到的东西”提炼出来，在解新题时直接塞给模型用。**
>
> ---
>
> 所以，这段话其实在讨论如何更聪明地“喂”给大语言模型示例，而不是简单粗暴地罗列。
>
> - **传统方法**：直接扔给它几个例子。
> - **新方法1**：让一个更强的模型帮忙创造完美的例子。
> - **新方法2**：把例子用特殊方式标记出来，并让模型重点关注。
> - **新方法3**：把例子所蕴含的“知识”提炼成精华，直接注入模型。

##### Demonstration Ordering

对选定的演示示例进行排序，也是演示组织中的一个重要环节。Lu et al. (2022) 已证明，顺序敏感性是各类模型普遍存在的一个问题。为解决该问题，早前的研究提出了一些无需训练的演示排序方法。其中，Liu et al. (2022) 根据示例与输入的相似度进行排列，将最相似的示例置于最末尾。Lu et al. (2022) 引入了全局和局部熵指标，并发现这些指标与上下文学习 (ICL) 的性能呈正相关。因此，他们利用熵指标来确定最优的演示顺序。此外，ICCL (Liu et al., 2024b) 建议将演示按由简到难的顺序排列，从而在推理过程中逐步提升演示示例的复杂度。

#### Instruction Formatting

格式化演示的常见方法是直接将示例 (*x*<sub>1</sub>, *y*<sub>1</sub>), ... , (*x*<sub>k</sub>, *y*<sub>k</sub>) 与一个模板 T 进行拼接。然而，在某些需要复杂推理的任务（如数学应用题和常识推理）中，仅凭 k 个演示很难让模型学会从 *x*<sub>i</sub> 到 *y*<sub>i</sub> 的映射。尽管模板工程 (template engineering) 在提示 (prompting) 领域已有研究 (Liu et al., 2023c)，但一些研究者旨在通过指令 *I* 来描述任务，从而为 ICL 设计更优的演示格式。Honovich et al. (2023) 发现，给定少量演示示例，LLM 能够自行生成任务指令。考虑到 LLM 的生成能力，Zhou et al. (2023c) 提出了**自动提示工程师 (Automatic Prompt Engineer)**，用于自动生成和筛选指令。为了进一步提升自动生成指令的质量，有多种策略被提出，它们利用 LLM 对自身生成的内容进行迭代优化 (bootstrap) (Wang et al., 2023f; Chen et al., 2024)。此外，**思维链 (Chain-of-Thought, CoT)** (Wei et al., 2022c) 在输入和输出之间引入中间推理步骤，以增强模型的解题和理解能力。近期的研究进展也同样强调了增强模型逐步推理过程的重要性 (Zhang et al., 2023c; Wang et al., 2022a; Zhou et al., 2023a)。

#### Scoring Function

评分函数决定了如何将语言模型的预测结果转换为对特定答案的似然度 (likelihood) 估计。**直接法 (Direct method)** 使用由模型词汇表中的词元 (token) 所表示的候选答案的条件概率 (Brown et al., 2020)。概率最高的答案被选为最终答案，但这种方法要求答案词元必须位于输入序列的末尾，从而限制了模板的设计。**困惑度 (Perplexity, PPL)** 是另一个常用指标，它计算整个输入序列 _S_<sub>j</sub> = {*C*, *s*(*x*, *y*<sub>j</sub> , _I_ )} 的句子困惑度，该序列包含了演示示例 *C*、输入查询 *x* 和候选标签 *y*<sub>j</sub> 的词元。PPL 评估的是整个句子的概率，消除了词元位置的限制，但需要额外的计算开销。Min et al. (2022a) 提出使用**信道模型 (Channel Model)** 来反向计算条件概率，即估计在给定标签的条件下输入查询的似然度。这种方法要求语言模型生成输入中的每一个词元，在训练数据不平衡的情况下有可能提升性能。我们在表 3 中总结了这三种评分函数。请注意，在表 3 中，“效率” 指的是语言模型的推理延迟；“覆盖范围” 反映了方法是利用输入序列中局部还是全部词元位置的输出概率；“稳定性” 则表示上下文学习能力是否容易受到演示示例变化的影响。

| Method  |               Target                | Efficiency | Coverage | Stability |
| :-----: | :---------------------------------: | :--------: | :------: | :-------: |
| Direct  |  _M_ ( _y_<sub>j</sub> \| C, _x_)   |    +++     |    +     |     +     |
|   PPL   |        PPL (_S_<sub>j</sub>)        |     +      |   +++    |     +     |
| Channel | _M_ ( _x_ \| _C_, _y_<sub>j</sub> ) |     +      |    +     |    ++     |

**表3：不同评分函数的总结。** “覆盖率”（Coverage）指的是任务覆盖范围。关于“效率”（Efficiency）和“稳定性”（Stability）这两个指标的定性结果，分别在表4和表5中有详细说明。

[^1]: [A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234)
